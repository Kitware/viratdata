---
layout: page
title: Mevadata
permalink: /
---

<base href="{{ site.baseurl }}/" />

<!-- Slider Start -->
<section id="slider">
  <div class="container">
    <div class="row">
      <div class="block">
        <h1 class="animated fadeInUp">The VIRAT Video Dataset</h1>
        <p class="animated fadeInUp">The VIRAT Video Dataset is designed to be realistic, 
		natural and challenging for video surveillance domains in terms of its 
		resolution, background clutter, diversity in scenes, and human activity/event 
		categories than existing action recognition datasets. It has become a benchmark 
		dataset for the computer vision community.</p>
      </div>
    </div>
  </div>
</section>
<!-- Wrapper Start -->
<section id="intro">
  <div class="container">
    <div class="row">
      <div class="col-md-5 col-sm-12">
        <div class="block">
          <div class="section-title">
            <h2>About the VIRAT Video dataset</h2>
          </div>
          <p>Compared to existing datasets,
		  the VIRAT dataset has the following distinguishing characteristics:
		<ul>
			<li><strong>Realism and natural scenes</strong>: Data was collected in natural scenes 
			showing people performing normal actions in standard contexts, 
			with uncontrolled, cluttered backgrounds. There are frequent incidental
			movers and background activities. Actions performed by directed actors 
			were minimized; most were actions performed by the general population.</li>
			<li><strong>Diversity</strong>: Data was collected at multiple sites distributed throughout 
			the USA. A variety of camera viewpoints and resolutions were included, and 
			actions are performed by many different people.</li>
			<li><strong>Quantity</strong>: Diverse types of human actions and human-vehicle interactions
			are included, with a large number of examples (>30) per action class.</li>
			<li><strong>Wide range of resolution and frame rates</strong>: Many applications such as video 
			surveillance operate across a wide range of spatial and temporal resolutions. 
			The dataset is designed to capture these ranges, with 2–30Hz frame rates and 
			10–200 pixels in person-height. The dataset provides both the original videos 
			with HD quality and downsampled versions both spatially and temporally.</li>
			<li><strong>Ground and Aerial Videos</strong>: Both ground camera videos and aerial videos
			are collected released as part of VIRAT Video Dataset.</li>
		</ul>
		The VIRAT Video Dataset contains two broad categories of activities (single-object and two-objects) 
		which involve both human and vehicles. Details of included activities, and annotation formats 
		may differ per release. Relevant information can be found from each release information.</p>
		  <h3>Additional Data and Annotations</h3>
			<p>The large-scale <strong>Multiview Extended Video with Activities (MEVA)</strong>
			dataset features more than 250 hours of ground camera video, with additional resources such as UAV video, camera models,
			and a subset of 12.5 hours of annotated data. The dataset is designed for activity detection in multi-camera environments. 
			It was created on the Intelligence Advanced Research Projects Activity (IARPA) 
			Deep Intermodal Video Analytics (DIVA) program to support DIVA performers and the broader research community. The dataset and
			annotations are available at the <a href="http://mevadata.org/">mevadata.org</a> site. </p>
			
          <h3>Release News</h3>
		  <p><strong>November 2019</strong>: New annotations performed on the IARPA DIVA program on the VIRAT
		  Public Dataset are now available. The full dataset was reannotated for additional
		  activity classes according to the activity definitions available here.</p>
          <p><strong>December 29, 2016</strong>: Aerial annotations status: Annotating the aerial 
		  video proved extremely challenging, and we were unable to complete the annotations on the 
		  original contract. We are actively pursuing promising funding opportunities and hope to 
		  have an update soon.</p>
		  <p><strong>2012 Jan 11th</strong>: Version 2.0 of the VIRAT Public Dataset is 
		  updated with Aerial video subsets. Currently, only videos are available. </p>
		  <p><strong>2011 Oct 4th</strong>: Version 2.0 of the VIRAT Public Dataset is
		  released with Ground video subsets. The main characteristics of this new version are as follows:
		  <ul>
		  <li>All videos are Stationary Ground Videos</li>
		  <li>Large amount of data: total ~8.5 hours of HD videos</li>
		  <li>Total 12 event types annotated, from videos from 11 different outdoor scenes</li>
		  <li>Includes suggested evaluation metrics and methodologies (data folds for cross-validation etc)</li></p>
		  <p>Release 2.0 is described in a PDF available <a href="https://data.kitware.com/api/v1/file/56f581c88d777f753209c9ce/download">here</a>. </p>
		    </div>
  </div>
</section>

<section id="feature">
<div class="container">
  <div class="row">
  <div class="col-md-6 col-md-offset-6">
    <div id="getting-data">
      <h2>ACCESSING AND USING THE VIRAT VIDEO DATASET</h2>
      <p>The new annotations follow the  Annotation Guidelines as described in this PDF. It describes the 
	  additional activity classes and object types. The full set of annotations are available for use with the data.<br>
	  <a href="annotation-location-link-here"
      class="button-primary">New Annotations</a>
	  
	  <p>Release 2.0 of the VIRAT Video Dataset is described in a PDF available <a href="https://data.kitware.com/api/v1/file/56f581c88d777f753209c9ce/download">here</a>.
	  It is available for download at the link below. </p>
	  <br><a href="https://data.kitware.com/#collection/56f56db28d777f753209ba9f/folder/56f57e748d777f753209bed6"
        class="button-primary">Browse and Download Release 2.0</a>
	
		<p>Prior releases of the data are also available, including <a href="http://www.viratdata.org/virat/virat_archive1.html">Release 1.0. <br>
		<a href="https://data.kitware.com/#collection/56f56db28d777f753209ba9f"
      class="button-primary">Browse and Download Prior Releases</a>
		<h3>Citation Information</h3>
		<p>If you make use of the VIRAT Video Dataset, please use the following citation (with release version information):<br>
		"A Large-scale Benchmark Dataset for Event Recognition in Surveillance Video" by Sangmin Oh, Anthony Hoogs, 
		Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee, Saurajit Mukherjee, J.K. Aggarwal, Hyungtae Lee, 
		Larry Davis, Eran Swears, Xiaoyang Wang, Qiang Ji, Kishore Reddy, Mubarak Shah, Carl Vondrick, Hamed Pirsiavash, 
		Deva Ramanan, Jenny Yuen, Antonio Torralba, Bi Song, Anesco Fong, Amit Roy-Chowdhury, and Mita Desai, in 
		Proceedings of IEEE Comptuer Vision and Pattern Recognition (CVPR), 2011.</p>
      </div>
    </div>
  </div>
</div>
</section>

<section id="service">
  <div class="container">
    <div class="row">
      <div class="section-title">
        <h2>Contact and Acknowledgements</h2>
        <p>A dedicated e-mail list to share information and report issues about the dataset can be found <a href="http://public.kitware.com/mailman/listinfo/viratdata">
		here</a>. Please subscribe the list for announcements and Q/&A</p>
		<p><strong>Acknowledgements</strong><br>
		The VIRAT Video Dataset collection work is supported by Defense Advanced Research Projects Agency
		(DARPA) under Contract No. HR0011-08-C-0135. Any opinions, findings and conclusions or 
		recommendations expressed in this material are those of the authors and do not necessarily 
		reflect the views of DARPA.</p>
		<p><strong>Disclaimer</strong><br>
		The views expressed are those of the author and do not reflect the official policy or position
		of the Department of Defense or the U.S. Government.</p>


      </div>
    </div>
    <div class="row ">
      <div class="col-sm-6 col-md-4">
        <div class="service-item">
          <i class="ion-checkmark-circled"></i>
          <h4>Download data</h4>
          See the instructions <a href="#getting-data">here</a> to
  obtain the data.
        </div>
      </div>
      <div class="col-sm-6 col-md-4">
        <div class="service-item">
          <i class="ion-qr-scanner"></i>
          <h4>View annotation exemplars</h4>
          <p>Download and review short clips of visualized annotations for each activity type.</p>
          <a href="https://data.kitware.com/#item/5cddc85b8d777f072bb0b7aa" class="button-primary">Download exemplars</a>
        </div>
      </div>
      <div class="col-sm-6 col-md-4">
        <div class="service-item">
          <i class="ion-code-download"></i>
          <h4>Review Annotation Guidelines</h4>
          <p>Download the current activity definitions. These should guide which activities
            and objects are annotated.</p>
          <a href="https://gitlab.kitware.com/meva/meva-data-repo/blob/master/documents/MEVA-Annotation-Definitions.pdf" class="button-primary">Download guidelines</a>
        </div>
      </div>
      <div class="col-sm-6 col-md-4">
        <div class="service-item">
          <i class="ion-qr-scanner"></i>
          <h4>Generate annotations</h4>
<p>Generate schema like <a
  href="https://gitlab.kitware.com/meva/meva-data-repo/tree/master/documents/schemas">
  these</a> based on the format <a href="https://gitlab.kitware.com/meva/meva-data-repo/blob/master/documents/MEVA_Annotation_JSON.pdf">described
  here</a> as part of our <a href="https://gitlab.kitware.com/meva/meva-data-repo">annotation git repository</a>.
       </div>
      </div>
      <div class="col-sm-6 col-md-4">
        <div class="service-item">
          <i class="ion-upload"></i>
          <h4>Contribute annotations</h4>
          <p>Contributing your annotations will increase the utility
  of the MEVA KF1 dataset for everyone. Please clone our <a
  href="https://gitlab.kitware.com/meva/meva-data-repo">annotation git
  repository</a> and file a merge request to have your annotations
  pooled back into the master branch.
</p>
        </div>
      </div>

	  </div>
    </div>
</section>

{% comment %}
<!-- CALL TO ACTION START
<section id="call-to-action">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <div class="block">
          <h2>We design delightful digital experiences.</h2>
          <p>Read more about what we do and our philosophy of design. Judge for yourself The work and results we’ve achieved for other clients, and meet our highly experienced Team who just love to design.</p>
          <a class="btn btn-default btn-call-to-action" href="#" >Tell Us Your Story</a>
        </div>
      </div>
    </div>
  </div>
</section> CALL TO ACTION END -->

<!-- CONTENT START
<section id="testimonial">
  <div class="container">
    <div class="row">
      <div class="section-title text-center">
        <h2>Fun Facts About Us</h2>
        <p>Far far away, behind the word mountains, far from the countries Vokalia and Consonantia, there live the blind texts. Separated they live in Bookmarksgrove right at the coast of the Semantics</p>
      </div>
    </div>
    <div class="row">
      <div class="col-md-6">
        {% if site.data.funfacts.size > 0 %}
        <div class="block">
          {% for ff in site.data.funfacts %}
          <ul class="counter-box clearfix">
            <li>
              <div class="block">
                <i class="{{ ff.icon }}"></i>
                <h4 class="counter">{{ ff.counter }}</h4>
                <span>{{ ff.text }}</span>
              </div>
            </li>
            {% endfor %}
          </ul>
        </div>
        {% endif %}
      </div>
      <div class="col-md-6">
        {% if site.data.testimonials.size > 0 %}
        <div class="testimonial-carousel">
          <div id="testimonial-slider" class="owl-carousel">
            {% for tm in site.data.testimonials %}
            <div>
                <img src="img/cotation.png" alt="IMG">
                <p>{{ tm.testimonial }}</p>
                <div class="user">
                  <img src="{{ tm.image }}" alt="Pepole">
                  <p><span>{{ tm.name }}</span> {{ tm.title }}</p>
                </div>
            </div>
            {% endfor %}
          </div>
        </div>
        {% endif %}
      </div>
    </div>
  </div>
</section> CONTENT END-->
{% endcomment %}
