---
layout: page
title: VIRAT Video Data
permalink: /
---

<base href="{{ site.baseurl }}/" />

<!-- Slider Start -->
<section id="slider">
  <div class="container">
    <div class="row">
      <div class="block">
        <h1 class="animated fadeInUp">The VIRAT Video Dataset</h1>
        <p class="animated fadeInUp">The VIRAT Video Dataset is designed to be realistic, 
		natural and challenging for video surveillance domains in terms of its 
		resolution, background clutter, diversity in scenes, and human activity/event 
		categories than existing action recognition datasets. It has become a benchmark 
		dataset for the computer vision community.</p>
      </div>
    </div>
  </div>
</section>
<!-- Wrapper Start -->
<section id="intro">
  <div class="container">
    <div class="row">
      <div class="col-md-5 col-sm-12">
        <div class="block">
          <div class="section-title">
            <h2>About the VIRAT Video dataset</h2>
          </div>
          <p>The VIRAT video dataset distinguishes itself on the following characteristics.</p>
		  <p><strong>Realism and natural scenes</strong>: Data was collected in natural scenes 
			showing people performing normal actions in standard contexts, 
			with uncontrolled, cluttered backgrounds. There are frequent incidental
			movers and background activities. Actions performed by directed actors 
			were minimized; most were actions performed by the general population.</p>
			<p><strong>Diversity</strong>: Data was collected at multiple sites distributed throughout 
			the USA. A variety of camera viewpoints and resolutions were included, and 
			actions are performed by many different people.</p>
			<p><strong>Quantity</strong>: Diverse types of human actions and human-vehicle interactions
			are included, with a large number of examples (>30) per action class.</p>
			<p><strong>Wide range of resolution and frame rates</strong>: Many applications such as video 
			surveillance operate across a wide range of spatial and temporal resolutions. 
			The dataset is designed to capture these ranges, with 2–30Hz frame rates and 
			10–200 pixels in person-height. The dataset provides both the original videos 
			with HD quality and downsampled versions both spatially and temporally.</p>
			<p><strong>Ground and Aerial Videos</strong>: Both ground camera videos and aerial videos
			are collected released as part of VIRAT Video Dataset.</p>
		<p>The VIRAT Video Dataset contains two broad categories of activities (single-object and two-objects) 
		which involve both human and vehicles. Details of included activities, and annotation formats 
		may differ per release. Relevant information can be found from each release information.</p>
		  <h3>Additional Data and Annotations</h3>
			<p>The large-scale <strong>Multiview Extended Video with Activities (MEVA)</strong>
			dataset features more than 250 hours of ground camera video, with additional resources such as UAV video, camera models,
			and a subset of 12.5 hours of annotated data. The dataset is designed for activity detection in multi-camera environments. 
			It was created on the Intelligence Advanced Research Projects Activity (IARPA) 
			Deep Intermodal Video Analytics (DIVA) program to support DIVA performers and the broader research community. The dataset and
			annotations are available at the <a href="http://mevadata.org/">mevadata.org</a> site. </p>
			
          <h3>Release News</h3>
          <p><strong>December 29, 2016</strong>: Aerial annotations status: Annotating the aerial 
		  video proved extremely challenging, and we were unable to complete the annotations on the 
		  original contract. We are actively pursuing promising funding opportunities and hope to 
		  have an update soon.</p>
		  <p><strong>2012 Jan 11th</strong>: Version 2.0 of the VIRAT Public Dataset is 
		  updated with Aerial video subsets. Currently, only videos are available. </p>
		  <p><strong>2011 Oct 4th</strong>: Version 2.0 of the VIRAT Public Dataset is
		  released with Ground video subsets. All videos, totalling ~8.5 hours of HD data, are Stationary Ground Videos. 
		  The annotations are for 12 event types, annotated in videos from 11 different outdoor scenes. The release also 
		  includes suggested evaluation metrics and methodologies (data folds for cross-validation etc.)</p>
		  <p>Release 2.0 is described in a PDF available <a href="https://data.kitware.com/api/v1/file/56f581c88d777f753209c9ce/download">here</a>. </p>
		    </div>
  </div><!-- .col-md-5 close -->
  <div class="col-md-7 col-sm-12">
        <div class="block">
          <video controls width="100%" style="margin: 32px 0 20px;">
            <source src="video/VIRAT_S_010204_05_000856_000890.mp4" type="video/mp4">
            Sorry, your browser doesn't support embedded videos.
          </video>
        </div>
      </div><!-- .col-md-7 close -->
	</div>
   </div>
</section>

<section id="feature">
<div class="container">
  <div class="row">
  <div class="col-md-6 col-md-offset-6">
    <div id="getting-data">
      <h2>ACCESSING AND USING THE VIRAT VIDEO DATASET</h2>
	  <p>Release 2.0 of the VIRAT Video Dataset is described in a PDF available <a href="https://data.kitware.com/api/v1/file/56f581c88d777f753209c9ce/download">here</a>.
	  It is available for download at the link below. </p>
	  <br><a href="https://data.kitware.com/#collection/56f56db28d777f753209ba9f/folder/56f57e748d777f753209bed6"
        class="button-primary">Browse and Download Release 2.0</a>
			<p>Prior releases of the data are also available, including <a href="http://www.viratdata.org/virat/virat_archive1.html">Release 1.0. You can also browse and 
		download <a href="https://data.kitware.com/#collection/56f56db28d777f753209ba9f"> Prior Releases</a>.</p>

		<h3>Citation Information</h3>
		<p>If you make use of the VIRAT Video Dataset, please use the following citation (with release version information):<br>
		"A Large-scale Benchmark Dataset for Event Recognition in Surveillance Video" by Sangmin Oh, Anthony Hoogs, 
		Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee, Saurajit Mukherjee, J.K. Aggarwal, Hyungtae Lee, 
		Larry Davis, Eran Swears, Xiaoyang Wang, Qiang Ji, Kishore Reddy, Mubarak Shah, Carl Vondrick, Hamed Pirsiavash, 
		Deva Ramanan, Jenny Yuen, Antonio Torralba, Bi Song, Anesco Fong, Amit Roy-Chowdhury, and Mita Desai, in 
		Proceedings of IEEE Comptuer Vision and Pattern Recognition (CVPR), 2011.</p>
      </div>
    </div>
  </div>
</div>
</section>

<section id="service">
  <div class="container">
    <div class="row">
      <div class="section-title">
        <h2>Contact and Acknowledgements</h2>
        <p>A dedicated e-mail list to share information and report issues about the dataset can be found <a href="http://public.kitware.com/mailman/listinfo/viratdata">
		here</a>. Please subscribe the list for announcements and questions and answers. </p>
		<p><strong>Acknowledgements</strong><br>
		The VIRAT Video Dataset collection work is supported by Defense Advanced Research Projects Agency
		(DARPA) under Contract No. HR0011-08-C-0135. Any opinions, findings and conclusions or 
		recommendations expressed in this material are those of the authors and do not necessarily 
		reflect the views of DARPA.</p>
		<p><strong>Disclaimer</strong><br>
		The views expressed are those of the author and do not reflect the official policy or position
		of the Department of Defense or the U.S. Government.</p>


      </div>
    </div>
    </div>
</section>

{% comment %}
<!-- CALL TO ACTION START
<section id="call-to-action">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <div class="block">
          <h2>We design delightful digital experiences.</h2>
          <p>Read more about what we do and our philosophy of design. Judge for yourself The work and results we’ve achieved for other clients, and meet our highly experienced Team who just love to design.</p>
          <a class="btn btn-default btn-call-to-action" href="#" >Tell Us Your Story</a>
        </div>
      </div>
    </div>
  </div>
</section> CALL TO ACTION END -->

<!-- CONTENT START
<section id="testimonial">
  <div class="container">
    <div class="row">
      <div class="section-title text-center">
        <h2>Fun Facts About Us</h2>
        <p>Far far away, behind the word mountains, far from the countries Vokalia and Consonantia, there live the blind texts. Separated they live in Bookmarksgrove right at the coast of the Semantics</p>
      </div>
    </div>
    <div class="row">
      <div class="col-md-6">
        {% if site.data.funfacts.size > 0 %}
        <div class="block">
          {% for ff in site.data.funfacts %}
          <ul class="counter-box clearfix">
            <li>
              <div class="block">
                <i class="{{ ff.icon }}"></i>
                <h4 class="counter">{{ ff.counter }}</h4>
                <span>{{ ff.text }}</span>
              </div>
            </li>
            {% endfor %}
          </ul>
        </div>
        {% endif %}
      </div>
      <div class="col-md-6">
        {% if site.data.testimonials.size > 0 %}
        <div class="testimonial-carousel">
          <div id="testimonial-slider" class="owl-carousel">
            {% for tm in site.data.testimonials %}
            <div>
                <img src="img/cotation.png" alt="IMG">
                <p>{{ tm.testimonial }}</p>
                <div class="user">
                  <img src="{{ tm.image }}" alt="Pepole">
                  <p><span>{{ tm.name }}</span> {{ tm.title }}</p>
                </div>
            </div>
            {% endfor %}
          </div>
        </div>
        {% endif %}
      </div>
    </div>
  </div>
</section> CONTENT END-->
{% endcomment %}
